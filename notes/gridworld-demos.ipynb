{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:48.356125Z",
     "start_time": "2019-11-05T18:03:48.351098Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:48.377269Z",
     "start_time": "2019-11-05T18:03:48.360150Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.632376Z",
     "start_time": "2019-11-05T18:03:48.380106Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from progress.bar import Bar\n",
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.table import Table\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **States**: boxes in a rectangular grid\n",
    "- **Immediate reward at time $t$**: $r_t=-1$\n",
    "- **Terminal states (where we would like the agent to be as fast as possible)**: \n",
    "    - in terminal states, $r_t=0$\n",
    "    - the box on the top left corner and the box on the bottom right corner\n",
    "- **Policy to under which we will evaluate the values of states**: \n",
    "    - a random policy, $\\pi(s,a)=0.25$, for all action $a$ available in state $s$ ($a \\in A(s)$)\n",
    "    - in this example, $a \\in A(s) = \\{\\uparrow, \\downarrow, \\leftarrow, \\rightarrow\\}$ for $s \\in S+$\n",
    "- **Rules / Environment**:\n",
    "    - Deterministic:\n",
    "        - $p(s', r|s, a)=1$ if and only\n",
    "            - agent can use $a$ to move from $s$ to $s'$, which implies that\n",
    "                - $s'$ must be right next to and touching $s$, since $a$ only moves the agent for one step in the horizontal or the vertical direction\n",
    "        - otherwise $p(s', r|s, a)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment hyper-parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.641012Z",
     "start_time": "2019-11-05T18:03:49.635091Z"
    }
   },
   "outputs": [],
   "source": [
    "WORLD_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.683803Z",
     "start_time": "2019-11-05T18:03:49.643738Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_terminal(state):\n",
    "    x, y = state\n",
    "    return (x == 0 and y == 0) or (x == WORLD_SIZE - 1 and y == WORLD_SIZE - 1)\n",
    "\n",
    "def is_trap(state):\n",
    "    x, y = state\n",
    "    return (x == 0 and y == 2) or (x == 1 and y == 2) or (x == 1 and y == 3) or (x == 2 and y == 3) or (x == 2 and y == 2) or (x == 2 and y == 1) or (x == 1 and y == 2) or (x == 2 and y == 0)\n",
    "\n",
    "def is_gold(state):\n",
    "    x, y = state\n",
    "    return (x == 1 and y == 1) or (x == 0 and y == 1) or (x == 1 and y == 0)\n",
    "\n",
    "def is_wall(state):\n",
    "    i, j = state\n",
    "    if i == 2 and j in list(range(4, 8)):\n",
    "        return True\n",
    "    elif i in list(range(3, 7)) and j == 2:\n",
    "        return True\n",
    "    elif i in list(range(3, 6)) and j == 7:\n",
    "        return True\n",
    "    elif i == 7 and j in list(range(2, 8)):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the environment responses to an agent's action when the agent is in some given state.\n",
    "- GridWorld is a deterministic environment, that is, $p(s', r|s, a)$ is either 1 (if s' in accessible in s AND reachable by a) or 0 (if s' is not accessible in s OR not reachable by a) for any 4-tuple (s', r, s, a).\n",
    "- Therefore, s' and r can be directly determined from s and a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.698260Z",
     "start_time": "2019-11-05T18:03:49.688844Z"
    }
   },
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    \"\"\"Given a tuple of state and action, output a tuple of immediate reward and next state\"\"\"\n",
    "    \n",
    "    # check if the agent has already reached the terminal states; if yes, keep it there\n",
    "    if is_terminal(state):\n",
    "        return state, 0\n",
    "\n",
    "    next_state = (np.array(state) + action).tolist()\n",
    "    x, y = next_state\n",
    "\n",
    "    # check if the agent is outside the grid; if yes, put it back to its last position in the grid\n",
    "    if x < 0 or x > WORLD_SIZE - 1 or y < 0 or y > WORLD_SIZE - 1 or is_wall(next_state):\n",
    "        next_state = state\n",
    "            \n",
    "    if is_trap(next_state):\n",
    "        reward = -2\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.710820Z",
     "start_time": "2019-11-05T18:03:49.703202Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# left, up, right, down\n",
    "ACTIONS = [np.array([0, -1]), # left\n",
    "           np.array([-1, 0]), # up\n",
    "           np.array([0, 1]),  # right\n",
    "           np.array([1, 0])]  # down\n",
    "ACTION_PROB = [25/100, 25/100, 25/100, 25/100]  # modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.723684Z",
     "start_time": "2019-11-05T18:03:49.713127Z"
    }
   },
   "outputs": [],
   "source": [
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.731090Z",
     "start_time": "2019-11-05T18:03:49.726038Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, a in enumerate(ACTIONS):\n",
    "    if np.array_equal(a, np.array([0, -1])):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "The optimal value function is a result of the optimal behavior of the agent:\n",
    "$$v_{*}(s)=\\max_{\\pi}v_{\\pi}(s)=v_{\\text{argmax}_{\\pi}v_{\\pi}}=v_{\\pi_*} \\text{ for all } s \\in S+$$\n",
    "\n",
    "Yet the agent can only behave optimally when it is guided by the optimal value function:\n",
    "$$\\pi_{*}(s)=\\text{argmax}_{a} \\sum_{s', r}p(s', r|s, a)v_{*}(s') \\text{ for all } s \\in S+$$\n",
    "\n",
    "This inter-dependence is similar for the responsibilities and parameters when using EM to optimize for a GMM. We can see that the formula for responsibilities involve the parameters and vice versa. The reason why the EM algorithm works is because one iteration of the E step and the M step is guaranteed to improve the log-likehood, until a local or global maximum.\n",
    "\n",
    "Similarly, we can prove that one iteration of policy evaluation and policy improvement (together called policy iteration) is guaranteed to improve the value function and therefore the policy. Furthermore, we can show that the value function stops improving if only if it becomes equal to the optimal value function. Therefore, policy iteration guarantees not only the monotonic increase of the value function but also its convergence to a global maximum (a property that EM for GMM does not have)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='em1.png' width=400>\n",
    "<img src='em2.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation Algorithm, for estimating $V=v_{\\pi}$, essential for Policy Improvement Algorithm\n",
    "Without an accurate estimation of value function of the previous policy, it makes no sense to act greedily with respect to it and hope that we find a better policy. Similarly, in EM for GMM, it makes no sense to update the parameters without first computing the responsibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.756365Z",
     "start_time": "2019-11-05T18:03:49.734096Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_policy(policy=None, first_round=True, niters=None, in_place=True, discount=1.0):\n",
    "    \n",
    "    # \"intialize V(s) = 0, for all s in S+\"\n",
    "    new_state_values = np.zeros((WORLD_SIZE, WORLD_SIZE))  \n",
    "    \n",
    "    # do not play active role, just for recording the number of iterations before convergence\n",
    "    iteration = 0\n",
    "    \n",
    "    theta = 1e-4\n",
    "    while True:  # first loop (\"Loop:\")\n",
    "    \n",
    "        if in_place:\n",
    "            # when new_state_values get updated in line 29, it is immediately used\n",
    "            state_values = new_state_values\n",
    "        else:\n",
    "            # when new_state_values get updated in line 29, it is NOT immediately used\n",
    "            state_values = new_state_values.copy()\n",
    "            \n",
    "        old_state_values = state_values.copy() # new values\n",
    "\n",
    "        # second loop (\"Loop for each s in S:\")\n",
    "        # compute new values iteratively and put them in a grid\n",
    "        for i, j in itertools.product(range(WORLD_SIZE), range(WORLD_SIZE)):\n",
    "            if not is_wall((i, j)):\n",
    "                # everything here corresponds to \"V(s) <- ...\"\n",
    "                value = 0\n",
    "                if first_round:\n",
    "                    for k, action in enumerate(ACTIONS):\n",
    "                        (next_i, next_j), reward = step([i, j], action)\n",
    "                        value += ACTION_PROB[k] * (reward + discount * state_values[next_i, next_j])\n",
    "                else:\n",
    "                    action = policy[i][j]\n",
    "                    (next_i, next_j), reward = step([i, j], action)\n",
    "                    value = 1 * (reward + discount * state_values[next_i, next_j])\n",
    "                new_state_values[i, j] = value\n",
    "\n",
    "        # instead of iteratively computing deltas and comparing them\n",
    "        # first compute elementwise difference between the grid containing new values from the grid containing old values\n",
    "        # then apply max to get the maximum delta value\n",
    "        # corespond to \"delta <- 0\", \"v <- V(s)\" and \"delta <- max(delta, |v-V(s)|)\"\n",
    "        max_delta_value = abs(old_state_values - new_state_values).max()\n",
    "        \n",
    "        # do not play active role, just for recording the number of iterations before convergence\n",
    "        iteration += 1\n",
    "        \n",
    "        # stop if precision requirement is met\n",
    "        if niters is None:\n",
    "            if max_delta_value < theta:\n",
    "                break\n",
    "        else:\n",
    "            if iteration == niters:\n",
    "                break\n",
    "    \n",
    "    return new_state_values, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.786419Z",
     "start_time": "2019-11-05T18:03:49.760589Z"
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(values):\n",
    "    new_policy = np.zeros_like(values)\n",
    "    new_policy_indices = np.zeros_like(values)\n",
    "    new_policy = new_policy.tolist()\n",
    "    for i in range(values.shape[0]):\n",
    "        for j in range(values.shape[1]):\n",
    "            if not is_wall((i, j)):\n",
    "            \n",
    "                value_here = values[i, j]\n",
    "\n",
    "                if j-1 >= 0 and not is_wall((i, j-1)):\n",
    "                    value_left = values[i, j-1]\n",
    "                else:\n",
    "                    value_left = value_here\n",
    "\n",
    "                if i+1 <= WORLD_SIZE-1 and not is_wall((i+1, j)):\n",
    "                    value_down = values[i+1, j]\n",
    "                else:\n",
    "                    value_down = value_here\n",
    "\n",
    "                if j+1 <= WORLD_SIZE-1 and not is_wall((i, j+1)):    \n",
    "                    value_right = values[i, j+1]\n",
    "                else:\n",
    "                    value_right = value_here\n",
    "\n",
    "                if i-1 >= 0 and not is_wall((i-1, j)):\n",
    "                    value_up = values[i-1, j]\n",
    "                else:\n",
    "                    value_up = value_here\n",
    "\n",
    "                immediate_rewards = np.array([step((i, j), a)[1] for a in ACTIONS]) \n",
    "                surrounding_values = np.array([value_left, value_up, value_right, value_down])\n",
    "                \n",
    "                action_index = np.argmax(immediate_rewards + surrounding_values)\n",
    "                action_here = ACTIONS[action_index]\n",
    "\n",
    "                new_policy[i][j] = action_here\n",
    "                new_policy_indices[i, j] = action_index\n",
    "    return new_policy, new_policy_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:49.810695Z",
     "start_time": "2019-11-05T18:03:49.789636Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def draw_image(image):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = image.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Add cells\n",
    "    for (i, j), val in np.ndenumerate(image):\n",
    "        tb.add_cell(i, j, width, height, text=val,\n",
    "                    loc='center', facecolor='white')\n",
    "\n",
    "        # Row and column labels...\n",
    "    for i in range(len(image)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "    ax.add_table(tb)\n",
    "\n",
    "def figure_4_1(eval_policy_only=False):\n",
    "    # While the author suggests using in-place iterative policy evaluation,\n",
    "    # Figure 4.1 actually uses out-of-place version.\n",
    "    #_, asycn_iteration = eval_policy(in_place=True)\n",
    "    total_iter = 0\n",
    "    \n",
    "    if eval_policy_only:\n",
    "        values, sync_iter = eval_policy(policy=None, first_round=True, niters=1000, in_place=False)\n",
    "        total_iter = sync_iter\n",
    "        policy=None\n",
    "    else:\n",
    "        first_round=True\n",
    "        policy=None\n",
    "        for i in range(20):\n",
    "            print(f'Policy Iteration - Round {i}')\n",
    "            values, sync_iter = eval_policy(policy=policy, first_round=first_round, niters=50, in_place=False)\n",
    "            total_iter += sync_iter\n",
    "            policy, _ = improve_policy(values); first_round = False\n",
    "        draw_image(np.round(values, decimals=2))\n",
    "    \n",
    "    #print('In-place: {} iterations'.format(asycn_iteration))\n",
    "    print('Synchronous: {} iterations'.format(total_iter))\n",
    "\n",
    "    plt.savefig('figure_4_1.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:17.391685Z",
     "start_time": "2019-11-05T18:21:15.590268Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_values, opt_policy = figure_4_1(eval_policy_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:18.837831Z",
     "start_time": "2019-11-05T18:21:17.456465Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_values, opt_policy = figure_4_1(eval_policy_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivated by: https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:20.158906Z",
     "start_time": "2019-11-05T18:21:20.149863Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_values(values):\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    for i in range(values.shape[0]):\n",
    "        for j in range(values.shape[1]):\n",
    "            if is_wall((i, j)):\n",
    "                values[i, j]=np.nan\n",
    "    im = ax.matshow(values)\n",
    "    for (i, j), z in np.ndenumerate(values):\n",
    "        if is_trap((i, j)):\n",
    "            ax.text(j, i, 'TRAP\\n(r=-2)\\n{:0.2f}'.format(z), ha='center', va='center', color='red')\n",
    "        elif is_wall((i, j)):\n",
    "            ax.text(j, i, '', ha='center', va='center')\n",
    "        elif values[i, j] < -9:\n",
    "            ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center', color='white')\n",
    "        else:\n",
    "            ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:20.999993Z",
     "start_time": "2019-11-05T18:21:20.672636Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_values(opt_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods for Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks to do (2019.10.20):\n",
    "- give `eval_policy_mc` a new name (done)\n",
    "- go through code, change var names (done)\n",
    "- go through code, add comments, including how they relate to the current chapter (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:53.366007Z",
     "start_time": "2019-11-05T18:03:53.346114Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_action_probas(epsilon):\n",
    "    \n",
    "    vs = np.random.uniform(size=(WORLD_SIZE, WORLD_SIZE)) \n",
    "    _, greedy_action_indices = improve_policy(vs)\n",
    "    \n",
    "    action_probas = np.zeros((10, 10, 4))  # think of this as a cake ...\n",
    "    for (row, col), greedy_action_index in np.ndenumerate(greedy_action_indices):\n",
    "        action_probas[row, col] = epsilon / 4\n",
    "        action_probas[row, col, int(greedy_action_index)] = 1 - epsilon + epsilon / 4\n",
    "        \n",
    "    return action_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:53.379881Z",
     "start_time": "2019-11-05T18:03:53.372193Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_action_probas(action_probas, qsa, s_t, epsilon):\n",
    "    greedy_action_index = np.argmax(qsa[s_t])\n",
    "    action_probas[s_t[0], s_t[1]] = epsilon / 4\n",
    "    action_probas[s_t[0], s_t[1], greedy_action_index] = 1 - epsilon + epsilon / 4\n",
    "    return action_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:53.390863Z",
     "start_time": "2019-11-05T18:03:53.385074Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_states_actions():\n",
    "    S = [(i, j) for i, j in itertools.product(range(WORLD_SIZE), range(WORLD_SIZE)) if not is_wall((i, j))]\n",
    "    S = np.array(S)\n",
    "    A = np.array(ACTIONS)\n",
    "    return S, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:53.399734Z",
     "start_time": "2019-11-05T18:03:53.393809Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_action_from_state(state, actions, action_probas):\n",
    "    action_index = np.random.choice([0, 1, 2, 3], p=normalize_probas(action_probas[state[0], state[1]]))\n",
    "    action = actions[action_index]\n",
    "    return action, action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:53.407021Z",
     "start_time": "2019-11-05T18:03:53.403305Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_probas(probas):\n",
    "    return probas / probas.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:03:53.419469Z",
     "start_time": "2019-11-05T18:03:53.409350Z"
    }
   },
   "outputs": [],
   "source": [
    "qsa = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:31:06.542198Z",
     "start_time": "2019-11-05T19:31:06.523464Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_qsa_monteCarlo(num_traversals=50000, \n",
    "                            epsilon=0.1, \n",
    "                            decreasing_epsilon=False,\n",
    "                            optimal_v=None,\n",
    "                            method='mc'):\n",
    "\n",
    "    ########## Initialization ##########\n",
    "    \n",
    "    qsa = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))   # initialize value for each state-action pair\n",
    "    action_probas = init_action_probas(epsilon)  # connects q(s, a) and policy\n",
    "    \n",
    "    S, A = get_states_actions()\n",
    "    \n",
    "    # initialize a sum of returns and a counter of times sampled for each state-action pair\n",
    "    total_returns = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))\n",
    "    total_occurrences = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))\n",
    "    \n",
    "    # not included in the algorithm, only used for collecting stats\n",
    "    precisions = []\n",
    "\n",
    "    ##########\n",
    "    \n",
    "    # randomly picking the starting state-action pair for each traversal\n",
    "    a_indices = np.random.randint(len(A), size=num_traversals)\n",
    "    As = A[a_indices]\n",
    "    \n",
    "    # TRAVERSAL LOOP\n",
    "    eps = epsilon\n",
    "    if method == 'mc' and epsilon < 0.5: print('epsilon is too small, might take too long to run...')\n",
    "    for n in log_progress(list(range(num_traversals)), every=100, name='No. Traversals Attempted'):\n",
    "            \n",
    "        # MC/Sarsa: initialize S\n",
    "        while True:\n",
    "            s_index = np.random.choice(len(S))\n",
    "            s = S[s_index]\n",
    "            x, y = tuple(s)\n",
    "            if not is_wall(s) and not (x <= 4 and y <= 4): break\n",
    "    \n",
    "        # MC: obtain the starting state-action pair for this traversal\n",
    "        # Sarsa: choose A from S using policy derived from Q (e.g., epsilon-greedy) [but does not matter too much here]\n",
    "        a = As[n]\n",
    "        action_index = a_indices[n]\n",
    "        \n",
    "        if method == 'mc':  # first-visit method\n",
    "        \n",
    "            # one traversal / one sampling round\n",
    "            episode_a_indices, episode_sa, episode_r = [], [], []\n",
    "            while True:\n",
    "\n",
    "                episode_sa.append((tuple(s), tuple(a))); episode_a_indices.append(action_index)  # record state-action pair\n",
    "                s, r = step(s, a); episode_r.append(r) # move and record immediate reward\n",
    "                action_index = np.random.choice([0, 1, 2, 3], p=normalize_probas(action_probas[s[0], s[1]]))  # pick a new action probabilistically\n",
    "                a = A[action_index]\n",
    "                \n",
    "                if is_terminal(s): break\n",
    "\n",
    "            # by definition, the value of a state-action pair is the expected cumulative reward the agent can gain by starting from it\n",
    "            # now, in Monte Carlo, we simple replace the \"expected cumulative reward\" with \"empirical mean of cumulative reward\"\n",
    "\n",
    "            cumulative_reward = 0\n",
    "            for t in reversed(range(len(episode_r))):\n",
    "                \n",
    "                cumulative_reward = cumulative_reward + episode_r[t]  # this cumulative reward can be interpreted differently depending on when we enter the if statement below\n",
    "\n",
    "                if episode_sa[t] not in episode_sa[:t]:\n",
    "                    \n",
    "                    (s_t, a_t), action_index = episode_sa[t], episode_a_indices[t] # first visit of state-action pair (s_t, a_t)\n",
    "\n",
    "                    # this following step cannot be written as gamma * total_return[(*s_t, i)] + cumulative_reward\n",
    "                    # may have to do with the convergence of Monte Carlo Methods in general\n",
    "\n",
    "                    sa_pair_index = (*s_t, action_index)\n",
    "                    total_returns[sa_pair_index] += cumulative_reward; total_occurrences[sa_pair_index] += 1\n",
    "                    qsa[sa_pair_index] =  total_returns[sa_pair_index] / total_occurrences[sa_pair_index]\n",
    "                    action_probas = update_action_probas(action_probas, qsa, s_t, eps)\n",
    "                    \n",
    "        elif method == 'sarsa':\n",
    "            \n",
    "            # hyperparameters\n",
    "            alpha = 0.5\n",
    "            lamb = 1.\n",
    "            \n",
    "            s = tuple(s)  # initialize S\n",
    "            a, a_index = get_action_from_state(s, A, action_probas)  # choose A from S using policy derived from Q (at the begining)\n",
    "            \n",
    "            depth = 0\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                if is_terminal(s): break\n",
    "                \n",
    "                s_prime, r = step(s, a); s_prime = tuple(s_prime)  # take action, observe S', R\n",
    "                a_prime, a_prime_index = get_action_from_state(s_prime, A, action_probas)  # choose A' from S' using policy dervied from Q\n",
    "                \n",
    "                qsa[(*s, a_index)] += \\\n",
    "                    alpha * (r + lamb * qsa[(*s_prime, a_prime_index)] - qsa[(*s, a_index)])\n",
    "                action_probas = update_action_probas(action_probas, qsa, s, eps)\n",
    "                s = s_prime; a = a_prime\n",
    "                \n",
    "                depth += 1\n",
    "    \n",
    "        if decreasing_epsilon:\n",
    "            eps = epsilon / (n+1)\n",
    "        \n",
    "        if n % 1000 == 0 and optimal_v is not None:\n",
    "            qsa_max = np.max(qsa, axis=2)\n",
    "            errors = abs(qsa_max-optimal_v)\n",
    "            precision = np.max(errors[~np.isnan(errors)])\n",
    "            precisions.append(precision)\n",
    "        \n",
    "    return qsa, action_probas, precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-soft Methods / On-policy methods (will explain later what this means)\n",
    "\n",
    "Recall $\\epsilon$-greedy methods for solving multi-armed bandits problems, in which non-greedy actions are explored with a probability of $\\epsilon$. The same thing is happening here, except that now we have many states instead of just one states. Acting greedily now means acting greedily with respect to the value function (the cumulative reward into the future) instead of just immediate rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:33:02.170948Z",
     "start_time": "2019-11-05T19:32:22.511096Z"
    }
   },
   "outputs": [],
   "source": [
    "qsa_5, action_probas_5, precisions_5 = evaluate_qsa_monteCarlo(num_traversals=50000, epsilon=0.5, decreasing_epsilon=True, optimal_v=opt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:32:09.849143Z",
     "start_time": "2019-11-05T19:31:28.649784Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qsa_sarsa, action_probas_sarsa, precisions_sarsa = \\\n",
    "    evaluate_qsa_monteCarlo(num_traversals=50000, \n",
    "                            epsilon=0.1, \n",
    "                            decreasing_epsilon=False, \n",
    "                            optimal_v=opt_values, \n",
    "                            method='sarsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:17:44.992479Z",
     "start_time": "2019-11-05T18:17:42.395815Z"
    }
   },
   "outputs": [],
   "source": [
    "qsa_7, action_probas_7, precisions_7 = evaluate_qsa_monteCarlo(num_traversals=50000, epsilon=0.7, decreasing_epsilon=True, optimal_v=opt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T17:44:08.918504Z",
     "start_time": "2019-11-05T17:44:03.131156Z"
    }
   },
   "outputs": [],
   "source": [
    "qsa_10, action_probas_10, precisions_10 = evaluate_qsa_monteCarlo(num_traversals=50000, epsilon=1, decreasing_epsilon=True, optimal_v=opt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:33:22.849716Z",
     "start_time": "2019-11-05T19:33:22.512973Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(0, 50000, 1000), precisions_5, label='$\\epsilon=0.5$ with 1/n decay')\n",
    "# plt.plot(np.arange(0, 50000, 1000), precisions_7, label='$\\epsilon=0.7$ with 1/n decay')\n",
    "# plt.plot(np.arange(0, 50000, 1000), precisions_10, label='$\\epsilon=1.0$ with 1/n decay')\n",
    "# plt.plot(np.arange(0, 50000, 1000), precisions_sarsa, label='sarsa')\n",
    "plt.ylabel('Max Error', fontsize=15); plt.xlabel('Iterations of Monte Carlo', fontsize=15)\n",
    "plt.title('Precision of Estimation over Time', fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:34:10.708325Z",
     "start_time": "2019-11-05T19:34:10.691250Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_values_with_action_and_errors(qsa, action_probas, opt_policy):\n",
    "    \n",
    "    print('4-by-4 square on the top left corner is not accessible as start states.')\n",
    "    \n",
    "    qsa_max = np.max(qsa, axis=2)\n",
    "    optimal_action_indices = action_probas.argmax(axis=2)\n",
    "    \n",
    "    num_errors = 0\n",
    "    errors = np.zeros_like(optimal_action_indices)\n",
    "    for i, row in enumerate(opt_policy):\n",
    "        for j, policy in enumerate(row):\n",
    "            if not is_wall((i, j)):\n",
    "                if tuple(policy) != tuple(ACTIONS[optimal_action_indices[i, j]]):\n",
    "                    errors[i, j] = 1\n",
    "                    num_errors += 1\n",
    "\n",
    "    for i in range(qsa_max.shape[0]):\n",
    "            for j in range(qsa_max.shape[1]):\n",
    "                if is_wall((i, j)):\n",
    "                    qsa_max[i, j]=np.nan\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    im = ax.matshow(qsa_max)\n",
    "\n",
    "    actions_names = [\"left\", \"up\", \"right\", \"down\"]\n",
    "\n",
    "    for (i, j), action_index in np.ndenumerate(optimal_action_indices):\n",
    "        action_name = actions_names[action_index]\n",
    "        if is_wall((i, j)):\n",
    "            ax.text(j, i, '', ha='center', va='center', color='black')\n",
    "        elif is_terminal((i, j)):\n",
    "            ax.text(j, i, str(round(qsa_max[i, j], 2)), ha='center', va='center', color='black', fontsize=15)\n",
    "        else:\n",
    "            if errors[i, j] == 0:\n",
    "                if qsa_max[i, j] >= -9:\n",
    "                    text_color = 'black'\n",
    "                else:\n",
    "                    text_color = 'white'    \n",
    "\n",
    "            else:\n",
    "                text_color = 'red'\n",
    "            if is_trap((i, j)):\n",
    "                ax.text(j, i, f'{round(qsa_max[i, j], 2)}\\n{action_name}\\nTRAP', ha='center', va='center', color=text_color, fontsize=13)\n",
    "            else:\n",
    "                ax.text(j, i, f'{round(qsa_max[i, j], 2)}\\n{action_name}', ha='center', va='center', color=text_color, fontsize=13)\n",
    "\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Total number of errors:', num_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:34:13.129815Z",
     "start_time": "2019-11-05T19:34:12.759466Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_values_with_action_and_errors(qsa_5, action_probas_5, opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T16:53:43.728675Z",
     "start_time": "2019-11-05T16:53:43.373996Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_values_with_action_and_errors(qsa_7, action_probas_7, opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T16:53:48.126023Z",
     "start_time": "2019-11-05T16:53:47.775853Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_values_with_action_and_errors(qsa_10, action_probas_10, opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- In the book, there is a quote related to this, that is,\n",
    "\"All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to ﬁnd the optimal actions).\" \n",
    "\n",
    "- This dilemma will be solved by off-policy methods, which use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior.\n",
    "\n",
    "- In contrast to off-policy methods, $\\epsilon$-soft methods are called on-policy methods, which means that the policy that is learned about and the one that explores are the same policy (a special case of off-policy methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
